{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning\n",
    "\n",
    "**Machine learning** is a branch of Artificial Intelligence (AI), where an algorithm (or **model**) improves itself through **\"learning\" from data** and, as a result, becomes increasingly proficient at performing its task.\n",
    "\n",
    "The **process** of machine learning can be divided in the **7 steps** listed below:\n",
    "\n",
    "1. Collect data\n",
    "2. Prepare data (Preprocessing)\n",
    "3. Choosing a model\n",
    "4. Training\n",
    "5. Evaluation\n",
    "6. Hyperparameter tuning\n",
    "7. Prediction\n",
    "\n",
    "More details [here](https://livecodestream.dev/post/2020-06-02-7-steps-of-machine-learning/), [here](https://analyticsindiamag.com/the-7-key-steps-to-build-your-machine-learning-model/), [here](https://www.kdnuggets.com/2018/05/general-approaches-machine-learning-process.html)\n",
    "\n",
    "# Collect data\n",
    "\n",
    "Given a problem we want to solve, we have to **collect data** that we will use to feed our model. The **quality and quantity** of information we get are **very important**: they will directly impact the **performances** of our model, i.e. how well or badly our model will work.\n",
    "\n",
    "A *dataset* is a **list of samples**: each sample is composed by a **set of features** and a desired **output** (or target). The objective of Machine Learning is to predict the value of the target of a sample from the values of the other features.\n",
    "\n",
    "Data collection is an important task, but here we do not cover this topic: we will use [pre-collected data in scikit-learn](https://scikit-learn.org/stable/datasets/index.html) for our analyses.\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "Typically, **real-world data is dirty**: incomplete, inconsistent, inaccurate (contains errors or outliers), and sometimes lacks specific attribute values/trends. Data preprocessing tools help to **clean**, format, and organize the raw data, making it ready for Machine Learning models.\n",
    "\n",
    "There are several significant **steps in data preprocessing**, that depend on the kind of data to handle:\n",
    "\n",
    "1. Importing libraries\n",
    "2. Getting (and watching) data\n",
    "3. Handling missing values\n",
    "4. Encoding categorical features\n",
    "5. Feature aggregation\n",
    "6. Feature sampling (optional)\n",
    "7. Splitting the dataset\n",
    "8. Feature scaling for numerical features (depending on the algorithm)\n",
    "9. Dimensionality reduction (optional)\n",
    "\n",
    "More details [here](https://www.upgrad.com/blog/data-preprocessing-in-machine-learning/), [here](https://medium.com/better-programming/data-preprocessing-for-machine-learning-3822ace03ae6), [here](https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825), [here](https://medium.com/@prtk13061992/data-preprocessing-steps-in-python-for-any-machine-learning-algorithm-2d52b57fa098).\n",
    "\n",
    "Today we will cover all the steps of preprocessing in Python.\n",
    "\n",
    "## 1. Importing libraries\n",
    "\n",
    "There are predefined Python libraries that can perform specific data preprocessing jobs. The core Python libraries used for data preprocessing are:\n",
    "\n",
    "* **[NumPy](https://numpy.org/)**: is the fundamental package for **scientific calculation** in Python. Hence, it is used for inserting any type of mathematical operation in the code;\n",
    "* **[Pandas](https://pandas.pydata.org/)**: is a library for **data manipulation and analysis**, extensively used for importing and managing the datasets;\n",
    "* **[Matplotlib](https://matplotlib.org/)**: is a Python **2D plotting library**, used to plot any type of charts in Python. Plots made in matplotlib may look \"old\" from an aesthetic point of view, a better choice for data visualization is **[Seaborn](https://seaborn.pydata.org/)**, a library build on top of Matplotlib that offers sane choices for plot style and color defaults; Another good choice, expecially good if the visualization needs to be dynamic and published on the web is [plotly](https://plotly.com).\n",
    "* **[Sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)**: [sklearn](https://scikit-learn.org/stable/) is the fundamental library for Machine Learning (but not for Deep Learning!), and makes available many tools for **preprocessing data**.\n",
    "\n",
    "All these libraries are already available in the [Anaconda](https://www.anaconda.com/) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from sklearn.preprocessing import <name of the function>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting (and displaying) data\n",
    "\n",
    "We need to **import the dataset** (or the datasets) for our ML project. Usually a dataset is saved as a **CSV (Comma-Separated Values) file**, a text file that uses a comma to separate values. For importing data in python from a csv file we use the ```read_csv``` function of Pandas library (details [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)).\n",
    "\n",
    "We will load the \"housing\" dataset, which contains the following attributes:\n",
    "\n",
    "1. `longitude`: A measure of how far west a house is; a higher value is farther west\n",
    "2. `latitude`: A measure of how far north a house is; a higher value is farther north\n",
    "3. `housing_median_age`: Median age of a house within a block; a lower number is a newer building\n",
    "4. `total_rooms`: Total number of rooms within a block\n",
    "5. `total_bedrooms`: Total number of bedrooms within a block\n",
    "6. `population`: Total number of people residing within a block\n",
    "7. `households`: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. `median_income`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. `median_house_value`: Median house value for households within a block (measured in US Dollars)\n",
    "10. `ocean_proximity`: Location of the house w.r.t ocean/sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./datasets/housing.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a dataset, each column is a **feature**, while each row is a **sample** (or an **example**).\n",
    "\n",
    "After importing, we can have a look on the data by **visualizing** the values of some of their features. The visualization depends on the kind of feature:\n",
    "* **Categorical**: features whose values are taken from a **defined set of values**, For example days in a week (Monday, Tuesday, ..., Sunday) or Boolean set (True, False). The values, distribution, and dispersion of categorical variables are best understood with **bar plots**;\n",
    "* **Numerical**: features whose values are **continuous or integer-valued**, i.e. numbers. This kind of features can be visualized with **histograms**, or **scatter plots**, for visualizing relationships between two numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Categorical features: bar plot ###\\n\")\n",
    "sns.countplot(x=data[\"ocean_proximity\"]);\n",
    "plt.title(\"Number of samples according to ocean proximity\")\n",
    "plt.show()\n",
    "\n",
    "#######################################################\n",
    "\n",
    "print(\"### Numerical features ###\\n\")\n",
    "print(\"# Histogram\")\n",
    "\n",
    "sns.displot(data[\"median_income\"])\n",
    "plt.title(\"Distribution of median income\")\n",
    "plt.show()\n",
    "\n",
    "#######################################################\n",
    "\n",
    "print(\"# Scatterplot\")\n",
    "\n",
    "plt.plot(data[\"households\"], data[\"population\"], '.')\n",
    "plt.title(\"Number of households vs. Population\")\n",
    "#plt.yscale(\"log\")     # what is the difference with log scale?\n",
    "#plt.xscale(\"log\")     # what is the difference with log scale?\n",
    "plt.xlabel(\"Population\")\n",
    "plt.ylabel(\"Households\")\n",
    "plt.show()\n",
    "\n",
    "#######################################################\n",
    "\n",
    "plt.figure(figsize=(5,6))\n",
    "plt.scatter(data[\"longitude\"], data[\"latitude\"], s=data[\"median_income\"], c=data[\"median_income\"], alpha=0.2)\n",
    "plt.title(\"Map of median incomes\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these lectures we will also employ the toy datasets made available by sklearn in the `sklearn.datasets` package (details [here](https://scikit-learn.org/stable/datasets/index.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling missing values\n",
    "\n",
    "Sometimes real-world data are **incomplete**, that is we can have samples with missing values in some feature. We can handle these missing values by:\n",
    "* **deleting** the entire row/column with missing values;\n",
    "* **replacing** the values with some kind of aggregate value, like mean, median, mode (i.e. most frequent), or with a constant value.\n",
    "\n",
    "There is **no rule of thumb** to select a specific option, it depends on the data and the problem statement which is intended to solve. To select the best option, the knowledge of both data and the application are needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select samples with missing data in a particular feature\n",
    "data[data[[\"total_bedrooms\"]].isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select samples with missing data in any feature\n",
    "data[data.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In this dataset only the 'total_bedrooms' colummns contains NaN values\n",
    "notb = data.loc[:, data.columns != 'total_bedrooms']\n",
    "notb[notb.isnull().any(axis=1)].size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete samples or features\n",
    "\n",
    "If we use the **deleting** strategy:\n",
    "* for deleting **samples** with missing values we can use the `dropna` function of Pandas (details [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html));\n",
    "* for deleting **features** with missing values we can use the `drop` function of Pandas (details [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select samples without missing data: assign result to \"data\" for deleting samples with missing values\n",
    "data.dropna()\n",
    "#data = data.dropna()\n",
    "\n",
    "# # delete features with missing data: assign result to \"data\" for deleting features with missing values\n",
    "data.drop(columns=[\"total_bedrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace values\n",
    "\n",
    "If we decide for the **replacing** strategy:\n",
    "* we can use only Pandas code;\n",
    "* we can use the `SimpleImputer` transformer (details [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)), where the parameter `strategy` sets the replacing strategy (mean, median, most_frequent, constant), and the parameter `fill_value` sets the value to use in case of \"constant\" strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values\n",
    "\n",
    "# select value for missing values\n",
    "value_for_missing_values = data[[\"total_bedrooms\"]].mean().values[0]       # mean\n",
    "#                         data[[\"total_bedrooms\"]].median().values[0]      # median\n",
    "#                         data[[\"total_bedrooms\"]].mode().values[0][0]     # mode\n",
    "#                         10                                               # constant value\n",
    "\n",
    "# replace missing values in \"total_bedrooms\" feature\n",
    "data.loc[data[[\"total_bedrooms\"]].isnull().any(axis=1), \"total_bedrooms\"] = value_for_missing_values\n",
    "\n",
    "# No samples with missing data are now present\n",
    "data[data[[\"total_bedrooms\"]].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# get data again\n",
    "data = pd.read_csv(\"./datasets/housing.csv\")\n",
    "\n",
    "# replace missing values in \"total_bedrooms\" feature with \"mean\" strategy\n",
    "add_miss = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "data[\"total_bedrooms\"] = add_miss.fit_transform(data[[\"total_bedrooms\"]])\n",
    "\n",
    "# No samples with missing data are now present\n",
    "data[data[[\"total_bedrooms\"]].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding *categorical* features\n",
    "\n",
    "A Feature is **categorical** if its values are taken from a defined set of values (or classes). Categorical features can be divided in:\n",
    "* **Ordinal**: values have a natural implied order, even if the scale of difference is not defined, for example (\"Extra small\", \"Small\", \"Medium\", \"Large\", \"Extra large\");\n",
    "* **Nominal**: values without any implied order, for example colors (\"Red\", \"Green\", \"Blue\").\n",
    "\n",
    "Depending on the learning algorithm, **it may be necessary to encode categorical and ordinal features into a numerical representation**.\n",
    "\n",
    "Ordinal and nominal features can be encoded in different ways:\n",
    "* **Ordinal encoding**: for *ordinal* features, each value is mapped to a single numerical value; in this way we can inform the model that two nearby values are more similar than two distant ones. It is implemented in sklearn with the `OrdinalEncoder` method (details [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)).\n",
    "* **One-hot encoding**: for *nominal* features, each value is mapped on a vector, where only one attribute will be equal to 1 (hot), while the others will be 0 (cold). In sklearn it is implemented by means of the function `OneHotEncoder` (details [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding ordinal features using Pandas\n",
    "\n",
    "data[\"ocean_proximity_ord\"] =data[\"ocean_proximity\"].replace(\n",
    "    {'INLAND': 0, 'NEAR BAY': 1, 'NEAR OCEAN':2, '<1H OCEAN':3, 'ISLAND':4})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding ordinal features using sklearn\n",
    "\n",
    "# Refreshing the dataset\n",
    "data = pd.read_csv(\"./datasets/housing.csv\")\n",
    "\n",
    "\n",
    "### Using SKLearn\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "ord_features = ordinal_encoder.fit_transform(data[[\"ocean_proximity\"]])\n",
    "\n",
    "# converting to pandas and to the right type\n",
    "ord_features = pd.DataFrame(ord_features, columns=[\"ord_ocean_proximity\"])\n",
    "ord_features = ord_features.astype(\"int\")\n",
    "\n",
    "# add new features to dataset\n",
    "data = data.merge(ord_features, left_index=True, right_index=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding nominal features using Pandas\n",
    "\n",
    "# Refreshing the dataset\n",
    "data = pd.read_csv(\"./datasets/housing.csv\")\n",
    "\n",
    "ocean_prox = data[[\"ocean_proximity\"]]\n",
    "ocean_prox_1hot = pd.get_dummies(ocean_prox, prefix=\"ocean_proximity\")\n",
    "\n",
    "data.merge(ocean_prox_1hot, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding nominal features using SKLearn\n",
    "\n",
    "# Refreshing the dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data = pd.read_csv(\"./datasets/housing.csv\")\n",
    "\n",
    "\n",
    "nom_encoder = OneHotEncoder()\n",
    "nom_features = nom_encoder.fit_transform(data[[\"ocean_proximity\"]])\n",
    "# sparse matrix to list of arrays\n",
    "nom_features = nom_features.toarray()\n",
    "# list of arrays to dataframe\n",
    "nom_features = pd.DataFrame(nom_features, columns=nom_encoder.categories_[0].tolist())\n",
    "# change type of features: from float to int\n",
    "nom_features = nom_features.astype(\"int\")\n",
    "# add new features to dataset\n",
    "data = data.merge(nom_features, left_index=True, right_index=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature aggregation\n",
    "\n",
    "**Aggregation** provides a high-level view of the data, since the behaviour of groups or aggregates is more stable than individual data objects.\n",
    "\n",
    "Aggregation can be performed with the `groupby` function of Pandas (details [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)): such function is applied to a dataframe, and has the form\n",
    "\n",
    "```python\n",
    "df.groupby(<columns>).<aggregate function>\n",
    "```\n",
    "\n",
    "where \"columns\" are the columns to aggregate, and \"aggregate function\" is the statistics to apply in each group, such as `mean()`, `median()`, `sum()` or `count()`. It id usually followed by the function `reset_index` (details [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html)), that reset the index of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data_2 = pd.read_csv(\"./datasets/daily-min-temperatures.csv\")\n",
    "\n",
    "# preprocessing: add features from Date\n",
    "# remember: indexes starts from 0 and the last index in a range is not included\n",
    "data_2[\"Year\"] = [int(x[:4]) for x in data_2[\"Date\"]]\n",
    "data_2[\"Month\"] = [int(x[5:7]) for x in data_2[\"Date\"]]\n",
    "data_2[\"Day\"] = [int(x[8:]) for x in data_2[\"Date\"]]\n",
    "\n",
    "# print dataset\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.plot(range(len(data_2)), data_2[\"Temp\"])\n",
    "plt.ylim(0,30)\n",
    "plt.title(\"Raw data\")\n",
    "plt.show()\n",
    "\n",
    "# group data by year and month, by calculating the average\n",
    "data_2_grouped = data_2[[\"Year\", \"Month\", \"Temp\"]].groupby([\"Year\",\"Month\"]).mean().reset_index()\n",
    "\n",
    "# plot grouped data\n",
    "plt.plot(range(len(data_2_grouped)), data_2_grouped[\"Temp\"])\n",
    "plt.ylim(0,30)\n",
    "plt.title(\"Grouped data (by year/month)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data_3_grouped = data_2[[\"Year\", \"Month\", \"Temp\"]].groupby(\n",
    "    [\"Year\"]).mean().reset_index()\n",
    "\n",
    "plt.plot(range(len(data_3_grouped)), data_3_grouped[\"Temp\"])\n",
    "plt.ylim(0, 30)\n",
    "plt.title(\"Grouped data (by year)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature sampling (optional)\n",
    "\n",
    "**Sampling** is a method that allows us to **get information about the population** based on the statistics from a subset of the population (sample), without having to investigate every individual. The obective is to determine a population’s characteristics by directly observing only a portion (or sample) of the population.\n",
    "\n",
    "In Machine Learning, Sampling is employed:\n",
    "* for a **preliminary investigation** and/or analysis of the data;\n",
    "* when **obtaining** the entire set of data is too expensive or time consuming;\n",
    "* when **processing** the entire set of data is too expensive or time consuming.\n",
    "\n",
    "There are several techniques for sampling (a good list is available [here](https://www.analyticsvidhya.com/blog/2019/09/data-scientists-guide-8-types-of-sampling-techniques/)), here we show how to implement the basic ones:\n",
    "* **Simple Random Sampling**: the probability of selecting any particular item is equal. In a pandas dataframe, we can employ the `sample` function (details [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html)): for a random sampling **without replacement** (when an item is selected, it is removed from the population), we must set `replace=False`, while for a random sampling **with replacement** (when an item is selected, it is not removed from the population), we must set `replace=True`. Another important parameter is `random_state`, that sets the seed of the random generator;\n",
    "* **Stratified Sampling**: split the data into several partitions, then draw random samples from each partition. In this way, we are able to keep in the sample a desired proportion of examples from each group. We'll se later how to get a stratified sampling in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple random sampling\n",
    "\n",
    "# get 5 random samples without replacement\n",
    "data.sample(n=5, replace=False, random_state=0)\n",
    "\n",
    "# get 5 random samples with replacement\n",
    "data.sample(n=5, replace=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Difference between Sampling with or without replacement ###\\n\")\n",
    "\n",
    "print(\"Sampling without replacement from the top 10 samples of dataset: no repetitions\")\n",
    "data[:10].sample(n=5, replace=False, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sampling with replacement from the top 10 samples of dataset: possible repetitions\")\n",
    "data[:10].sample(n=5, replace=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Splitting the dataset \n",
    "\n",
    "In order to avoid underestimating the generalization error we cannot estimate it over the same data we used for training the model. In most cases, the dataset needs to be **split** into two (not overlapping) sets: a **train set**, for *training the model*, and a **test set**, for *evaluating the performances* of the model. \n",
    "\n",
    "The objective of such operation is to evaluate the performances of the trained model: we use the train set for training the ML model, then we feed the trained model with the samples in the test model, and we check how much the results of the prediction coincide with the real outputs. In this way, test samples represent real-world data, that have not been employed for training the model.\n",
    "\n",
    "The method `train_test_split` (details [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) is employed for splitting the dataset into train and test sets: the first parameter is the matrix of the values of features, and the secon parameter is the list of target values. the user should specify the ratio of train set (using the parameter `train_size`) or the ratio of test set (with the parameter `test_size`). In order to improve the performances of the model, we should always randomly shuffle the dataset before splitting (setting the `shuffle` parameter to `True`, and a random seed with `random_state`); it could be also useful to set the `stratify` parameter, that applies stratified sampling on the list of features passed as argument (the proportion of values in the two sets will be the same as the proportion of values provided to parameter). The function returns 4 variables: the matrix of train set, the matrix of test set, the list of train targets, the list of test targets.\n",
    "\n",
    "In our example, we would like to predict the house value according to the other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data.drop(columns=[\"ocean_proximity\", \"ord_ocean_proximity\", \"median_house_value\"]), data[\"median_house_value\"], test_size=0.25, shuffle=True, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print split datasets\n",
    "train_data\n",
    "#test_data\n",
    "#train_labels\n",
    "#test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature scaling for **numerical features**\n",
    "\n",
    "Several ML algorithms are **distance-based**, i.e. they employ the distance among samples in a N-dimensional space. The drawback of such algorithms is that distance-based models cause **features with larger values to tend to dominate the features with smaller values**.\n",
    "\n",
    "In order to solve this issue, we can scale into a fixed range the values of each numerical features. In other words, feature scaling permits to limit the range of variables so that you can compare them on common grounds.\n",
    "\n",
    "\n",
    "Scikit-learn makes available [several methods](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) for transforming data before processing them, but the common methods used in such task are:\n",
    "* **Standardization**: subtracts the mean of each observation and then divides it by the standard deviation, implemented in scikit-learn on the ```StandardScaler``` method (details [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)). This technique is more effective if the feature follows a Gaussian distribution; and if the algorithm you are using does make assumptions about your data having a Gaussian distribution (such as linear regression, logistic regression, PCA).\n",
    "* **Normalization**: scales data to a fixed range — usually 0 to 1 - by means of the function ```MinMaxScaler``` (details [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler))). It is effective when features have different ranges, and when the algorithm you are using does not make assumptions about the distribution of your data (k-Nearest Neighbors, SVM, Neural Networks).\n",
    "\n",
    "The feature scaling for numerical features must be applied **after splitting the dataset in train and test set**: the transformer must be trained on the train set only, in order to calculate mean and standard deviation of training samples, and then applied on both train and test sets. Test samples represent real-world data, then if we take the mean and variance of the whole dataset we will be introducing future information into the training explanatory variables:\n",
    "* use ```fit_transform()``` on the **training set**;\n",
    "* use ```transform()``` on the **test set**.\n",
    "\n",
    "Feature scaling for numerical features is not required (and thus not effective when applied) for models that don’t take a distance-based approach, such as tree-based models (Decision Trees and Random Forests) or probability-based models (Naive Bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# train transformer on train data, and transform them\n",
    "train_data[[\"population\"]] = scaler.fit_transform(train_data[[\"population\"]])\n",
    "# now the transformer is trained on train data, it can be applied on test data\n",
    "test_data[[\"population\"]] = scaler.transform(test_data[[\"population\"]])\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# train transformer on train data, and transform them\n",
    "scaler.fit(train_data[[\"total_rooms\"]])\n",
    "train_data[[\"total_rooms\"]] = scaler.transform(train_data[[\"total_rooms\"]])\n",
    "# now the transformer is trained on train data, it can be applied on test data\n",
    "test_data[[\"total_rooms\"]] = scaler.transform(test_data[[\"total_rooms\"]])\n",
    "\n",
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c59b82a6123c8ff06953925ada5dfe4e95122f4dab0e19d13d78acc3865b186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
